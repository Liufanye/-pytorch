# 构建神经网络

## 检测是否有cuda

```python
if torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"
print(device) # 输出cuda
```



## 定义类

```python
class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
        nn.Linear(28*28,512),
        nn.ReLU(),
        nn.Linear(512,512),
        nn.ReLU(),
        nn.Linear(512,10),
        )

    def forward(self,x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

model = NeuralNetwork().to(device)
print(model)
```

要使用模型，我们需要将输入数据传递给它。这会执行模型的`forward`以及一些[后台操作](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866)。请勿`model.forward()`直接调用！

在输入上调用模型会返回一个二维张量，其中 dim=0 对应每个类的 10 个原始预测值的每个输出，dim=1 对应每个输出的单个值。我们通过将其传递给模块实例来获取预测概率`nn.Softmax`。

```python
X = torch.rand(1,28,28,device=device)
logits = model(X)
pred_probab = nn.Softmax(dim=1)(logits) #指定在第一维度上计算
y_pred = pred_probab.argmax(1) #在第一维度上找最大值
print(y_pred)
```

输出：

```
tensor([9], device='cuda:0')
```

补充：为什么pred_probab = nn.Softmax(dim=1)(logits)为什么要写俩个()

`pred_probab = nn.Softmax(dim=1)(logits)` 中的两个括号是**先创建对象，再调用对象**的连续操作，这是 PyTorch 中使用模块的常见写法。我们可以拆分成两步来理解：

**第一个括号 `nn.Softmax(dim=1)`**
这一步是**创建 `Softmax` 类的实例对象**，并指定参数 `dim=1`（表示在第 1 维度上计算 Softmax）。等价于：

```
softmax_layer = nn.Softmax(dim=1)  # 创建对象，指定参数
```

**第二个括号 `(logits)`**
这一步是**调用刚创建的 `softmax_layer` 对象**，并将 `logits` 作为输入数据传入。等价于：

```
pred_probab = softmax_layer(logits)  # 调用对象，传入数据
```

合并写法的原因

将两步合并为 `nn.Softmax(dim=1)(logits)`，本质上是 “创建对象后立即使用，无需单独命名”，这样写更简洁，尤其适只需要使用一次该模块时。

例如：

```python
# 对图像做归一化
normalized = nn.Normalize(mean=0.5, std=0.5)(image)

# 对数据做池化
pooled = nn.MaxPool2d(kernel_size=2)(feature_map)
```



## 模型参数

神经网络中的许多层都是*参数化的*，即具有在训练过程中优化的相关权重和偏差。子类化`nn.Module`会自动跟踪模型对象中定义的所有字段，并使所有参数都可以通过模型的`parameters()`方法访问`named_parameters()`。

在这个例子中，我们迭代每个参数，并打印它的大小和它的值的预览。

```python
for name,param in model.named_parameters():
    print(f"Layer:{name} | Size:{param.size()} | Values:{param[:2]}")
```

输出：

```
Layer:linear_relu_stack.0.weight | Size:torch.Size([512, 784]) | Values:tensor([[ 0.0266,  0.0138, -0.0066,  ...,  0.0214,  0.0251, -0.0144],
        [-0.0255,  0.0140, -0.0233,  ...,  0.0124, -0.0264, -0.0137]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer:linear_relu_stack.0.bias | Size:torch.Size([512]) | Values:tensor([-0.0084, -0.0210], device='cuda:0', grad_fn=<SliceBackward0>)
Layer:linear_relu_stack.2.weight | Size:torch.Size([512, 512]) | Values:tensor([[-0.0317,  0.0284,  0.0319,  ..., -0.0059, -0.0356, -0.0146],
        [ 0.0032,  0.0373, -0.0085,  ...,  0.0133, -0.0197, -0.0235]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer:linear_relu_stack.2.bias | Size:torch.Size([512]) | Values:tensor([-0.0310, -0.0191], device='cuda:0', grad_fn=<SliceBackward0>)
Layer:linear_relu_stack.4.weight | Size:torch.Size([10, 512]) | Values:tensor([[-0.0036,  0.0333,  0.0307,  ..., -0.0252, -0.0090, -0.0225],
        [-0.0049,  0.0336, -0.0358,  ..., -0.0385, -0.0165, -0.0288]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer:linear_relu_stack.4.bias | Size:torch.Size([10]) | Values:tensor([-0.0195,  0.0317], device='cuda:0', grad_fn=<SliceBackward0>)
```

