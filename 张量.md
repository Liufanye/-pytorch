# 张量

张量是一种特殊的数据结构，与数组和矩阵非常相似。在 PyTorch 中，我们使用张量来编码模型的输入和输出，以及模型的参数。

张量与 NumPy 的 ndarray 类似，不同之处在于，张量可以在 GPU 或其他专用硬件上运行，以加速计算。如果您熟悉 ndarray，那么 Tensor API 对您来说应该很容易上手。如果您不熟悉，请继续阅读此 API 快速入门指南。

```python
import torch
import numpy as np
```



## 张量初始化

张量可以通过多种方式初始化。请看以下示例：

**直接来自数据**

张量可以直接从数据创建。数据类型会自动推断。

```
data = [[1, 2], [3, 4]]
x_data = torch.tensor(data)
```



**来自 NumPy 数组**

可以从 NumPy 数组创建张量（反之亦然 - 请参阅[使用 NumPy 的桥接](https://docs.pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#bridge-to-np-label)）。

```
np_array = np.array(data)
x_np = torch.from_numpy(np_array)
```



**从另一个张量：**

除非明确覆盖，否则新张量将保留参数张量的属性（形状、数据类型）。

```
x_ones = torch.ones_like(x_data) # retains the properties of x_data
print(f"Ones Tensor: \n {x_ones} \n")

x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data
print(f"Random Tensor: \n {x_rand} \n")
```



```
Ones Tensor:
 tensor([[1, 1],
        [1, 1]])

Random Tensor:
 tensor([[0.8160, 0.9160],
        [0.7947, 0.4319]])
```



**使用随机或恒定值：**

`shape`是张量维度的元组。在下面的函数中，它决定了输出张量的维数。

```
shape = (2, 3,)
rand_tensor = torch.rand(shape)
ones_tensor = torch.ones(shape)
zeros_tensor = torch.zeros(shape)

print(f"Random Tensor: \n {rand_tensor} \n")
print(f"Ones Tensor: \n {ones_tensor} \n")
print(f"Zeros Tensor: \n {zeros_tensor}")
```



```
Random Tensor:
 tensor([[0.5854, 0.0539, 0.1163],
        [0.5194, 0.6982, 0.0703]])

Ones Tensor:
 tensor([[1., 1., 1.],
        [1., 1., 1.]])

Zeros Tensor:
 tensor([[0., 0., 0.],
        [0., 0., 0.]])
```



------

## 张量属性

张量属性描述它们的形状、数据类型以及存储它们的设备。

```
tensor = torch.rand(3, 4)

print(f"Shape of tensor: {tensor.shape}")
print(f"Datatype of tensor: {tensor.dtype}")
print(f"Device tensor is stored on: {tensor.device}")
```



```
Shape of tensor: torch.Size([3, 4])
Datatype of tensor: torch.float32
Device tensor is stored on: cpu
```



------

## 张量运算

[这里](https://pytorch.org/docs/stable/torch.html)全面描述了 100 多个张量运算，包括转置、索引、切片、数学运算、线性代数、随机采样等 。

它们都可以在 GPU 上运行（通常速度比 CPU 更快）。如果您使用的是 Colab，请前往“编辑”>“笔记本设置”分配 GPU。

```
# We move our tensor to the GPU if available
if torch.cuda.is_available():
  tensor = tensor.to('cuda')
  print(f"Device tensor is stored on: {tensor.device}")
```



```
Device tensor is stored on: cuda:0
```



尝试列表中的一些操作。如果您熟悉 NumPy API，您会发现 Tensor API 非常容易使用。

**标准的类似 numpy 的索引和切片：**

```
tensor = torch.ones(4, 4)
tensor[:,1] = 0
print(tensor)
```



```
tensor([[1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.]])
```



**补充：**

列表与 PyTorch 张量切片的核心区别

| 特性         | Python 列表                 | PyTorch 张量                    |
| ------------ | --------------------------- | ------------------------------- |
| 多维切片语法 | 需嵌套索引（如`lst[i][j]`） | 支持逗号分隔（如`tensor[i,j]`） |
| 内存效率     | 切片返回新列表（复制数据）  | 切片返回视图（共享内存）        |
| 高级操作支持 | 不支持省略号（`...`）       | 支持省略号简化高维切片          |
| 步长灵活性   | 支持步长，但多维需多次操作  | 支持多维同时指定步长            |
| 可修改性     | 切片结果可直接修改          | 视图修改会影响原张量            |



例如：

```python
tensor = torch.ones(4,4)
tmp = tensor[:,1]
tmp[:] = 0
print(tmp)
print(tensor)
```

输出：

```python
tensor([0., 0., 0., 0.])
tensor([[1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.]])
```



```python
# 从图像张量中裁剪感兴趣区域（ROI）
image = torch.randn(3, 256, 256)  # 3通道，256×256像素
roi = image[:, 50:200, 50:200]    # 裁剪中间150×150区域
```



```python
# 截断长序列为固定长度
sequence = torch.randn(1000)  # 长度1000的序列
truncated = sequence[100:500]  # 取中间400个元素
```



```python
# 提取最后一个维度的部分元素
batch = torch.randn(32, 10, 5)  # 32个样本，每个含10个5维特征
features = batch[:, :, 2:4]     # 提取每个特征的第2-3维度
```



具体的例子（**很形象**）：
```python
import torch

# 1. 一维张量切片（类似列表）
tensor_1d = torch.tensor([0, 1, 2, 3, 4, 5])
print("一维切片:", tensor_1d[1:4])  # tensor([1, 2, 3])

# 2. 二维张量切片（3行4列）
tensor_2d = torch.tensor([
    [1, 2, 3, 4],
    [5, 6, 7, 8],
    [9, 10, 11, 12]
])

# 取第1-2行（索引0和1），第2-3列（索引1和2）
print("\n二维切片:")
print(tensor_2d[:2, 1:3])
# 输出:
# tensor([[2, 3],
#         [6, 7]])

# 3. 三维张量切片（2×3×4）
tensor_3d = torch.arange(24).reshape(2, 3, 4)  # 2个3×4矩阵
print("\n三维张量:")
print(tensor_3d)
# 输出:
# tensor([[[ 0,  1,  2,  3],
#          [ 4,  5,  6,  7],
#          [ 8,  9, 10, 11]],
# 
#         [[12, 13, 14, 15],
#          [16, 17, 18, 19],
#          [20, 21, 22, 23]]])

# 取第0个矩阵的第1-2行，所有列
print("\n三维切片:")
print(tensor_3d[0, 1:, :])
# 输出:
# tensor([[ 4,  5,  6,  7],
#         [ 8,  9, 10, 11]])

# 4. 特殊切片（省略号与步长）
print("\n省略号切片:")
print(tensor_3d[..., 1])  # 取所有维度的第1列
# 输出:
# tensor([[ 1,  5,  9],
#         [13, 17, 21]])

print("\n步长切片:")
print(tensor_2d[::2, ::2])  # 隔行隔列取元素
# 输出:
# tensor([[ 1,  3],
#         [ 9, 11]])
```





**连接张量**您可以使用`torch.cat`沿给定维度连接一系列张量。另请参阅[torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html)，这是另一个与 略有不同的张量连接操作`torch.cat`。

```
t1 = torch.cat([tensor, tensor, tensor], dim=1)
print(t1)
```



```
tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],
        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],
        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],
        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])
```



补充：

- 张量的维度（dim）可以理解为 “方向”：
  - 0 维（标量）：无维度
  - 1 维（向量）：只有 `dim=0`（长度方向）
  - 2 维（矩阵）：`dim=0`（行方向，**上下**）、`dim=1`（列方向，**左右**）
  - 3 维（如 `[C, H, W]` 的图像）：`dim=0`（通道方向）、`dim=1`（高度方向）、`dim=2`（宽度方向）
- `torch.cat()` 要求所有待拼接张量的**非拼接维度必须形状相同**，否则会报错。

```python
a = torch.tensor([[1,2,3],[4,5,6]]) #(2,3)
b= torch.tensor([[7,8,9],[10,11,12]]) #(2,3)

cat_dim0 = torch.cat([a,b],dim=0)
print(cat_dim0.shape) #(4,3)
print(cat_dim0)

cat_dim1 = torch.cat([a,b],dim=1)
print(cat_dim1.shape) #(2,6)
print(cat_dim1)
```

输出：

```
torch.Size([4, 3])
tensor([[ 1,  2,  3],
        [ 4,  5,  6],
        [ 7,  8,  9],
        [10, 11, 12]])
torch.Size([2, 6])
tensor([[ 1,  2,  3,  7,  8,  9],
        [ 4,  5,  6, 10, 11, 12]])
```





**张量相乘**

```
# This computes the element-wise product
print(f"tensor.mul(tensor) \n {tensor.mul(tensor)} \n")
# Alternative syntax:
print(f"tensor * tensor \n {tensor * tensor}")
```



```
tensor.mul(tensor)
 tensor([[1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.]])

tensor * tensor
 tensor([[1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.]])
```



这计算两个张量之间的矩阵乘法

```
print(f"tensor.matmul(tensor.T) \n {tensor.matmul(tensor.T)} \n")
# Alternative syntax:
print(f"tensor @ tensor.T \n {tensor @ tensor.T}")
```



```
tensor.matmul(tensor.T)
 tensor([[3., 3., 3., 3.],
        [3., 3., 3., 3.],
        [3., 3., 3., 3.],
        [3., 3., 3., 3.]])

tensor @ tensor.T
 tensor([[3., 3., 3., 3.],
        [3., 3., 3., 3.],
        [3., 3., 3., 3.],
        [3., 3., 3., 3.]])
```



**就地操作** 带有`_`后缀的操作是就地操作。例如：`x.copy_(y)`、`x.t_()`、 将更改为`x`。

```
print(tensor, "\n")
tensor.add_(5)
print(tensor)
```



```
tensor([[1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.]])

tensor([[6., 5., 6., 6.],
        [6., 5., 6., 6.],
        [6., 5., 6., 6.],
        [6., 5., 6., 6.]])
```



笔记

就地操作可以节省一些内存，但在计算导数时可能会出现问题，因为会立即丢失历史记录。因此，不建议使用就地操作。

------



## 与 NumPy 桥接

CPU 上的张量和 NumPy 数组可以共享其底层内存位置，更改其中一个也会更改另一个。

### 张量到 NumPy 数组

```
t = torch.ones(5)
print(f"t: {t}")
n = t.numpy()
print(f"n: {n}")
```



```
t: tensor([1., 1., 1., 1., 1.])
n: [1. 1. 1. 1. 1.]
```



张量的变化反映在 NumPy 数组中。

```
t.add_(1)
print(f"t: {t}")
print(f"n: {n}")
```



```
t: tensor([2., 2., 2., 2., 2.])
n: [2. 2. 2. 2. 2.]
```



### NumPy 数组转张量

```
n = np.ones(5)
t = torch.from_numpy(n)
```



NumPy 数组的变化反映在张量中。

```
np.add(n, 1, out=n)
print(f"t: {t}")
print(f"n: {n}")
```



```
t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
n: [2. 2. 2. 2. 2.]
```